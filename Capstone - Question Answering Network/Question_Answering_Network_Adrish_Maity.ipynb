{"cells":[{"metadata":{"id":"-Tz7wt7G_kLd"},"cell_type":"markdown","source":"**Question answering** comes in many forms. In this example, weâ€™ll look at the particular type of extractive QA that involves answering a question about a passage by highlighting the segment of the passage that answers the question. This involves fine-tuning a model which predicts a start position and an end position in the passage. We will use the Stanford Question Answering Dataset (SQuAD) 2.0.\n\nWe will start by downloading the data:"},{"metadata":{"id":"ZKgPO4AUDZYi"},"cell_type":"markdown","source":"## **Note :**"},{"metadata":{"id":"xAv2_F8VDL9t"},"cell_type":"markdown","source":"Please write your code in the cells with the \"**Your code here**\" placeholder."},{"metadata":{"id":"ockcf0NvAHy5"},"cell_type":"markdown","source":"## **Download SQuAD 2.0 Data**"},{"metadata":{"id":"6pmSMjQN_5zd"},"cell_type":"markdown","source":"Note : This dataset can be explored in the Hugging Face model hub (SQuAD V2), and can be alternatively downloaded with the ðŸ¤— NLP library with load_dataset(\"squad_v2\")."},{"metadata":{"id":"R1l0fVVFb4ne","trusted":true,"outputId":"89f36543-dda0-4546-f875-4d86326a5389"},"cell_type":"code","source":"!pip install transformers==4.0.1","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting transformers==4.0.1\n  Downloading transformers-4.0.1-py3-none-any.whl (1.4 MB)\n\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4 MB 1.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (20.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (3.0.10)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (2020.4.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (2.23.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (1.18.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (4.45.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (0.0.43)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.0.1) (1.14.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.0.1) (2.4.7)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.0.1) (1.25.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.0.1) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.0.1) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.0.1) (2.9)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (4.45.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.0.1) (0.14.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==4.0.1) (1.14.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.0.1) (2020.4.4)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.0.1) (7.1.1)\nCollecting tokenizers==0.9.4\n  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9 MB 7.0 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.9.3\n    Uninstalling tokenizers-0.9.3:\n      Successfully uninstalled tokenizers-0.9.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 3.5.1\n    Uninstalling transformers-3.5.1:\n      Successfully uninstalled transformers-3.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 1.2.2 requires transformers<3.6,>=3.4, but you have transformers 4.0.1 which is incompatible.\u001b[0m\nSuccessfully installed tokenizers-0.9.4 transformers-4.0.1\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"id":"2GlKOoe6pLmQ","trusted":true,"outputId":"699b17ab-c97b-4803-fcfe-c265d5e1f18e"},"cell_type":"code","source":"!mkdir squad\n!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad/train-v2.0.json\n!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad/dev-v2.0.json","execution_count":2,"outputs":[{"output_type":"stream","text":"--2021-01-04 08:51:47--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\nResolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.109.153, 185.199.111.153, ...\nConnecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 42123633 (40M) [application/json]\nSaving to: â€˜squad/train-v2.0.jsonâ€™\n\nsquad/train-v2.0.js 100%[===================>]  40.17M   120MB/s    in 0.3s    \n\n2021-01-04 08:51:48 (120 MB/s) - â€˜squad/train-v2.0.jsonâ€™ saved [42123633/42123633]\n\n--2021-01-04 08:51:49--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\nResolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.109.153, 185.199.108.153, 185.199.111.153, ...\nConnecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.109.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4370528 (4.2M) [application/json]\nSaving to: â€˜squad/dev-v2.0.jsonâ€™\n\nsquad/dev-v2.0.json 100%[===================>]   4.17M  --.-KB/s    in 0.07s   \n\n2021-01-04 08:51:49 (58.6 MB/s) - â€˜squad/dev-v2.0.jsonâ€™ saved [4370528/4370528]\n\n","name":"stdout"}]},{"metadata":{"id":"4Jv7ipy5AYUM"},"cell_type":"markdown","source":"Each split is in a structured json file with a number of questions and answers for each passage (or context). Weâ€™ll take this apart into parallel lists of contexts, questions, and answers (note that the contexts here are repeated since there are multiple questions per context):"},{"metadata":{"id":"IKMDLA8woiA8","trusted":true},"cell_type":"code","source":"import json\nfrom pathlib import Path\nfrom tqdm import tqdm\n\ndef read_squad(path):\n    \n    \n    # Your code here\n\n    path = Path(path)\n    with open(path, 'rb') as f:\n        squad_dict = json.load(f)\n\n    contexts = []\n    questions = []\n    answers = []\n    for group in tqdm(squad_dict['data']):\n        for passage in group['paragraphs']:\n            context = passage['context']\n            for qa in passage['qas']:\n                question = qa['question']\n                for answer in qa['answers']:\n                    contexts.append(context)\n                    questions.append(question)\n                    answers.append(answer)\n    \n    return contexts, questions, answers\n\ntrain_contexts, train_questions, train_answers = read_squad('squad/train-v2.0.json')\nval_contexts, val_questions, val_answers = read_squad('squad/dev-v2.0.json')\n","execution_count":3,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 442/442 [00:00<00:00, 6462.85it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 3300.30it/s]\n","name":"stderr"}]},{"metadata":{"id":"2bPtDyaFAtQm"},"cell_type":"markdown","source":"The contexts and questions are just strings. The answers are dicts containing the subsequence of the passage with the correct answer as well as an integer indicating the character at which the answer begins. In order to train a model on this data we need (1) the tokenized context/question pairs, and (2) integers indicating at which token positions the answer begins and ends.\n\nFirst, letâ€™s get the character position at which the answer ends in the passage (we are given the starting position). Sometimes SQuAD answers are off by one or two characters, so we will also adjust for that."},{"metadata":{"id":"nraxKhioA1rG","trusted":true},"cell_type":"code","source":"def add_end_idx(answers, contexts):\n    for answer, context in zip(answers, contexts):\n      \n        # Your code here\n        gold_text = answer['text']\n        start_idx = answer['answer_start']\n        end_idx = start_idx + len(gold_text)\n\n        # sometimes squad answers are off by a character or two â€“ fix this\n        if context[start_idx:end_idx] == gold_text:\n            answer['answer_end'] = end_idx\n        elif context[start_idx-1:end_idx-1] == gold_text:\n            answer['answer_start'] = start_idx - 1\n            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n        elif context[start_idx-2:end_idx-2] == gold_text:\n            answer['answer_start'] = start_idx - 2\n            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n\nadd_end_idx(train_answers, train_contexts)\nadd_end_idx(val_answers, val_contexts)","execution_count":4,"outputs":[]},{"metadata":{"id":"CmLsa8hQA2a7"},"cell_type":"markdown","source":"Now train_answers and val_answers include the character end positions and the corrected start positions. Next, letâ€™s tokenize our context/question pairs. ðŸ¤— Tokenizers can accept parallel lists of sequences and encode them together as sequence pairs."},{"metadata":{"id":"KdhZS6o0qMty","trusted":true,"outputId":"e048540d-25bf-406e-9036-79f831e741cd"},"cell_type":"code","source":"# !pip install transformers\nfrom transformers import DistilBertTokenizerFast\n# from transformers import DistilBertTokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n# Your code here\ntrain_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n\n# Your code here\nval_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab30686c6e564d08946bab81044e917e"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descriptiâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5381f3308a3408b9e45895e46cfe971"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"id":"DJdAikOIA_Zo"},"cell_type":"markdown","source":"Next we need to convert our character start/end positions to token start/end positions. When using ðŸ¤— Fast Tokenizers, we can use the <b>built in char_to_token()</b> method."},{"metadata":{"id":"YPHMz8RJ7t7G","trusted":true,"outputId":"1e508719-2945-4aec-f557-f93b2070ca63"},"cell_type":"code","source":"tokenizer.model_max_length","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"512"},"metadata":{}}]},{"metadata":{"id":"Kau7PrtIBAde","trusted":true},"cell_type":"code","source":"def add_token_positions(encodings, answers):\n    start_positions = []\n    end_positions = []\n    \n    # Your code here\n    for i in range(len(answers)):\n        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n        # if None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length - 1\n        if end_positions[-1] is None:\n            end_positions[-1] = tokenizer.model_max_length - 1\n\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\nadd_token_positions(train_encodings, train_answers)\nadd_token_positions(val_encodings, val_answers)","execution_count":7,"outputs":[]},{"metadata":{"id":"QLIp2RhABITo"},"cell_type":"markdown","source":"Our data is ready. Letâ€™s just put it in a PyTorch/TensorFlow dataset so that we can easily use it for training. In PyTorch, we define a custom Dataset class. In TensorFlow, we pass a tuple of (inputs_dict, labels_dict) to the from_tensor_slices method."},{"metadata":{"id":"cchIqVWpBI7X","trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\n# Your code here\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {key: train_encodings[key] for key in ['input_ids', 'attention_mask']},\n    {key: train_encodings[key] for key in ['start_positions', 'end_positions']}\n))\n\n# Your code here\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    {key: val_encodings[key] for key in ['input_ids', 'attention_mask']},\n    {key: val_encodings[key] for key in ['start_positions', 'end_positions']}\n))","execution_count":8,"outputs":[]},{"metadata":{"id":"OshKaYppBOtD"},"cell_type":"markdown","source":"Now we can use a DistilBert model with a QA head for training:"},{"metadata":{"id":"-sGzpUDvBPVU","trusted":true,"outputId":"1e177b71-6c16-400d-f5cc-ed281c267e5b"},"cell_type":"code","source":"from transformers import TFDistilBertForQuestionAnswering\n\n# Your code here\nmodel = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90374b9679094fc0a7fb2ecb41b6a341"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descriâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d0dd1b3191943eeb66b90669747051f"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForQuestionAnswering: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs', 'dropout_19']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","name":"stderr"}]},{"metadata":{"id":"2UGXo8SwBTeX"},"cell_type":"markdown","source":"The data and model are both ready to go. You can train the model with Trainer/TFTrainer exactly as in the sequence classification example above. If using native PyTorch, replace labels with start_positions and end_positions in the training example. If using Kerasâ€™s fit, we need to make a minor modification to handle this example since it involves multiple model outputs."},{"metadata":{"id":"lSwoxCD_BU_D","trusted":true,"outputId":"003878d6-0606-4e71-f209-009cfef7ab3c"},"cell_type":"code","source":"# Keras will expect a tuple when dealing with labels\n\n# Write your code here to replace labels with start_positions and end_positions in the training example\ntrain_dataset_2 = train_dataset.map(lambda x, y: (x, (y['start_positions'], y['end_positions'])))\n\n# Keras will assign a separate loss for each output and add them together. So we'll just use the standard CE loss\n# instead of using the built-in model.compute_loss, which expects a dict of outputs and averages the two terms.\n# Note that this means the loss will be 2x of when using TFTrainer since we're adding instead of averaging them.\n\n# Your code here\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.distilbert.return_dict = False # if using ðŸ¤— Transformers >3.02, make sure outputs are tuples\n\n# Your code here\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n\nmodel.compile(optimizer=optimizer, loss=loss) # can also use any keras loss fn\nmodel.fit(train_dataset_2.shuffle(1000).batch(16), epochs=3, batch_size=16)","execution_count":10,"outputs":[{"output_type":"stream","text":"Epoch 1/3\n5427/5427 [==============================] - 2884s 531ms/step - loss: 2.8652 - output_1_loss: 1.4924 - output_2_loss: 1.3728\nEpoch 2/3\n5427/5427 [==============================] - 2886s 532ms/step - loss: 1.8177 - output_1_loss: 0.9595 - output_2_loss: 0.8582\nEpoch 3/3\n5427/5427 [==============================] - 2886s 532ms/step - loss: 1.3259 - output_1_loss: 0.7056 - output_2_loss: 0.6203\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fbbc178ead0>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Save the model and tokenizer"},{"metadata":{"id":"KR0Y49tMvtot","trusted":true,"outputId":"96afc170-2491-4be9-a638-898d0c1d4867"},"cell_type":"code","source":"model.save_pretrained(\"models\")\ntokenizer.save_pretrained(\"tokenizers\")","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"('tokenizers/tokenizer_config.json',\n 'tokenizers/special_tokens_map.json',\n 'tokenizers/vocab.txt',\n 'tokenizers/added_tokens.json')"},"metadata":{}}]},{"metadata":{"id":"3MsWTfOEgBQo"},"cell_type":"markdown","source":"## Model Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace labels with start_positions and end_positions in the validation example\nval_dataset_2 = val_dataset.map(lambda x, y: (x, (y['start_positions'], y['end_positions'])))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate on the validation dataset\nmodel.evaluate(val_dataset_2.shuffle(1000).batch(16), batch_size=16)","execution_count":13,"outputs":[{"output_type":"stream","text":"1269/1269 [==============================] - 220s 173ms/step - loss: 2.8982 - output_1_loss: 1.4973 - output_2_loss: 1.4009\n","name":"stdout"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"[2.8981881141662598, 1.4972575902938843, 1.4009298086166382]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### evaluate using the squad processor"},{"metadata":{"trusted":true,"id":"Dx9YluxkgBQo","outputId":"a22daf83-9f14-4f41-c3cd-7ef6ee403736"},"cell_type":"code","source":"from transformers.data.processors.squad import SquadV2Processor\nprocessor = SquadV2Processor()\nexamples = processor.get_dev_examples(\"squad/\", filename=\"dev-v2.0.json\")","execution_count":14,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:08<00:00,  3.98it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true,"id":"sl108_SXgBQo"},"cell_type":"code","source":"question_id_example_index_mapping = {example.qas_id: i for i, example in enumerate(examples)}\nquestion_id_to_has_answer = {example.qas_id: bool(example.answers) for example in examples}\nanswer_question_ids = [question_id for question_id, has_answer in question_id_to_has_answer.items() if has_answer]\nno_answer_question_ids = [question_id for question_id, has_answer in question_id_to_has_answer.items() if not has_answer]\n\n# get prediction for a specific question\ndef get_pred(question_id):\n    question = examples[question_id_example_index_mapping[question_id]].question_text\n    context = examples[question_id_example_index_mapping[question_id]].context_text\n    \n    inputs = tokenizer.encode_plus(question, context, return_tensors='tf', truncation=True, padding=True)\n    \n    start_scores, end_scores = model(inputs)\n    \n    answer_start = tf.argmax(start_scores, axis=1).numpy()[0]\n    answer_end = (tf.argmax(end_scores, axis=1) + 1).numpy()[0]\n    \n    answer =  tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n    \n    return answer","execution_count":15,"outputs":[]},{"metadata":{"id":"dKoFv_ArEcC0","outputId":"ec9cb221-f98c-46b1-97d9-ff6f37ddfb16","trusted":true},"cell_type":"code","source":"\npredictions = {}\nfrom tqdm import tqdm\n# generate predictions for questions with answers\nfor i in tqdm(range(len(answer_question_ids))):    \n    prediction = get_pred(answer_question_ids[i])\n    example = examples[question_id_example_index_mapping[answer_question_ids[i]]]\n    predictions[answer_question_ids[i]] = prediction\n\n# generate predictions for questions with no answers\nfor i in tqdm(range(len(no_answer_question_ids))):\n    prediction = get_pred(no_answer_question_ids[i])\n    example = examples[question_id_example_index_mapping[no_answer_question_ids[i]]]\n    predictions[no_answer_question_ids[i]] = prediction","execution_count":16,"outputs":[{"output_type":"stream","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5928/5928 [05:33<00:00, 17.78it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5945/5945 [05:40<00:00, 17.45it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true,"id":"yq2JXqnTgBQr"},"cell_type":"code","source":"from transformers.data.metrics.squad_metrics import squad_evaluate","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"id":"qScmEKN2gBQt","outputId":"ba1dde3e-ae34-4d3d-c23c-01b43625c4d3"},"cell_type":"code","source":"# generate squad evaluation report on the default hyper parameter\nsquad_evaluate(examples, predictions)","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"OrderedDict([('exact', 21.8563126421292),\n             ('f1', 26.530124192115917),\n             ('total', 11873),\n             ('HasAns_exact', 31.140350877192983),\n             ('HasAns_f1', 40.50137728289351),\n             ('HasAns_total', 5928),\n             ('NoAns_exact', 12.598822539949538),\n             ('NoAns_f1', 12.598822539949538),\n             ('NoAns_total', 5945),\n             ('best_exact', 65.61947275330581),\n             ('best_exact_thresh', 0.0),\n             ('best_f1', 70.29328430329252),\n             ('best_f1_thresh', 0.0)])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate squad evaluation report on the a specific no_answer_probability_threshold\nsquad_evaluate(examples, predictions, no_answer_probability_threshold=-1.15)","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"OrderedDict([('exact', 50.07159100480081),\n             ('f1', 50.07159100480081),\n             ('total', 11873),\n             ('HasAns_exact', 0.0),\n             ('HasAns_f1', 0.0),\n             ('HasAns_total', 5928),\n             ('NoAns_exact', 100.0),\n             ('NoAns_f1', 100.0),\n             ('NoAns_total', 5945),\n             ('best_exact', 65.61947275330581),\n             ('best_exact_thresh', 0.0),\n             ('best_f1', 70.29328430329252),\n             ('best_f1_thresh', 0.0)])"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}